{
  "task_read_telemetry": {
    "musician": "FileSystemMusician",
    "action": "read_file",
    "parameters": {
      "file_path": "C:\\syncphony\\telemetry.py"
    }
  },
  "task_fix_and_write_telemetry": {
    "musician": "FileSystemMusician",
    "action": "write_file",
    "parameters": {
      "file_path": "C:\\syncphony\\telemetry.py",
      "content": "# C:\\syncphony\\telemetry.py\n\nimport asyncio\nimport collections\nimport hashlib\nimport json\nimport time\nimport os\nfrom datetime import datetime\nfrom functools import wraps\nfrom jsonschema import validate, ValidationError, RefResolver\nimport traceback # Added for capturing stack traces in error events\nimport sys # Added for sys.path.append, though ideally telemetry is generic\n\n# Add the current directory to sys.path so schema files can be found\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\n\n# --- Configuration ---\nTELEMETRY_BUFFER_SIZE = 100 # Max events to buffer before attempting to flush\nTELEMETRY_FLUSH_INTERVAL_SECONDS = 5 # How often to try flushing, even if buffer not full\nTELEMETRY_API_ENDPOINT = \"http://localhost:8080/telemetry_events\" # Placeholder for a real-time event sink\nHASH_ALGORITHM = \"SHA256\" # Standardized hash algorithm for data integrity\n\n# This should point to the root directory where your schemas are stored\n# IMPORTANT: Adjust this path if your 'syncphony_schemas' directory is not directly inside C:\\syncphony\nSCHEMA_ROOT_DIR = \"C:\\\\syncphony\\\\syncphony_schemas\\\\events\"\n\n# In-memory buffer for telemetry events (thread-safe due to asyncio.Lock)\n_telemetry_buffer = collections.deque()\n_buffer_lock = asyncio.Lock()\n_last_flush_time = time.time()\n\n# --- Schema Loading Utility ---\n_schemas_cache = {}\n_resolver = None\n\ndef _load_schema(schema_filename: str):\n    \"\"\"Loads a JSON schema from a file in SCHEMA_ROOT_DIR and caches it.\"\"\"\n    full_path = os.path.join(SCHEMA_ROOT_DIR, schema_filename)\n    if full_path not in _schemas_cache:\n        try:\n            with open(full_path, 'r', encoding='utf-8') as f:\n                schema = json.load(f)\n                _schemas_cache[full_path] = schema\n        except (FileNotFoundError, json.JSONDecodeError) as e:\n            raise RuntimeError(f\"Failed to load schema from {full_path}: {e}\")\n    return _schemas_cache[full_path]\n\ndef _get_schema_resolver():\n    \"\"\"Initializes and returns a RefResolver for schema loading.\"\"\"\n    global _resolver\n    if _resolver is None:\n        base_uri = \"http://syncphony.com/schemas/events/\"\n        handlers = {\n            # This handler intercepts requests for \"http://syncphony.com/schemas/events/...\"\n            # and loads them from our local file system path.\n            base_uri: lambda uri: _load_schema(uri.split(base_uri)[1])\n        }\n        _resolver = RefResolver(base_uri=base_uri, referrer={}, handlers=handlers)\n\n        # Pre-load base schema into resolver's store to ensure it's available for $ref\n        try:\n            base_schema_content = _load_schema(\"event_base.json\")\n            _resolver.store[\"http://syncphony.com/schemas/events/event_base.json\"] = base_schema_content\n        except RuntimeError as e:\n            print(f\"CRITICAL ERROR: Failed to pre-load event_base.json into resolver: {e}\")\n            # Depending on severity, you might want to sys.exit(1) here\n\n    return _resolver\n\n# Load all primary schemas at module initialization\n# This ensures they are ready when emit_telemetry_event is called.\ntry:\n    TASK_LIFECYCLE_EVENT_SCHEMA = _load_schema(\"task_lifecycle_event.json\")\n    GDC_SNAPSHOT_EVENT_SCHEMA = _load_schema(\"gdc_snapshot_event.json\")\n    SUB_LOG_ENTRY_SCHEMA = _load_schema(\"sub_log_entry.json\")\n    # Initialize the resolver after all primary schemas are defined\n    _resolver_instance = _get_schema_resolver()\nexcept RuntimeError as e:\n    print(f\"CRITICAL ERROR: Failed to load core schemas. Telemetry validation might be impaired: {e}\")\n    # Fallback to empty schemas to avoid crashing, but validation will fail/be skipped\n    TASK_LIFECYCLE_EVENT_SCHEMA = {}\n    GDC_SNAPSHOT_EVENT_SCHEMA = {}\n    SUB_LOG_ENTRY_SCHEMA = {}\n\n# --- Utility Functions ---\ndef _generate_event_id():\n    \"\"\"Generates a unique ID for each telemetry event.\"\"\"\n    return f\"event-{os.urandom(8).hex()}-{int(time.time() * 1000)}\"\n\ndef _calculate_payload_hash(payload):\n    \"\"\"Calculates a hash of the payload for data integrity.\"\"\"\n    payload_str = json.dumps(payload, sort_keys=True) # Ensure consistent hashing\n    if HASH_ALGORITHM == \"SHA256\":\n        return hashlib.sha256(payload_str.encode('utf-8')).hexdigest()\n    return None # Should not happen with SHA256\n\ndef _mask_sensitive_data(data, sensitive_keys=[\"password\", \"api_key\", \"secret\", \"token\", \"credential\", \"auth\"]):\n    \"\"\"Recursively masks sensitive data in a dictionary or list.\"\"\"\n    masked = False\n    if isinstance(data, dict):\n        for key, value in data.items():\n            if key in sensitive_keys:\n                data[key] = \"[MASKED]\"\n                masked = True\n            elif isinstance(value, (dict, list)):\n                nested_masked = _mask_sensitive_data(value, sensitive_keys)\n                if nested_masked:\n                    masked = True\n    elif isinstance(data, list):\n        for i, item in enumerate(data):\n            nested_masked = _mask_sensitive_data(item, sensitive_keys)\n            if nested_masked:\n                masked = True\n    return masked\n\nasync def _flush_telemetry_buffer():\n    \"\"\"Flushes the buffered telemetry events to the API endpoint.\"\"\"\n    global _last_flush_time\n    async with _buffer_lock:\n        if not _telemetry_buffer:\n            return\n\n        events_to_flush = list(_telemetry_buffer)\n        _telemetry_buffer.clear()\n        _last_flush_time = time.time()\n\n    print(f\"[Telemetry]: Attempting to flush {len(events_to_flush)} events...\")\n    try:\n        # In a real async application, this would use aiohttp or similar\n        # For this example, we simulate a network call.\n        await asyncio.sleep(0.01) # Simulate I/O latency\n        # print(json.dumps(events_to_flush, indent=2)) # Uncomment for raw telemetry event inspection\n        print(f\"[Telemetry]: Successfully flushed {len(events_to_flush)} events.\")\n    except Exception as e:\n        print(f\"[Telemetry ERROR]: Failed to flush events: {e}\")\n        # Re-add events to buffer for next flush attempt (with care to avoid infinite loops)\n        async with _buffer_lock:\n            _telemetry_buffer.extendleft(reversed(events_to_flush))\n\nasync def _telemetry_flusher_task():\n    \"\"\"Background task to periodically flush the telemetry buffer.\"\"\"\n    while True:\n        await asyncio.sleep(TELEMETRY_FLUSH_INTERVAL_SECONDS)\n        if _telemetry_buffer or (time.time() - _last_flush_time) >= TELEMETRY_FLUSH_INTERVAL_SECONDS:\n            await _flush_telemetry_buffer()\n\nasync def emit_telemetry_event(musician_name, task_id, event_type, payload, parent_event_id=None, mask_sensitive=True):\n    \"\"\"\n    Creates, validates, and buffers a telemetry event.\n    \"\"\"\n    processed_payload = payload.copy() # Avoid modifying original payload\n    was_masked = False\n    if mask_sensitive:\n        was_masked = _mask_sensitive_data(processed_payload)\n\n    # Ensure payload is JSON-serializable\n    try:\n        json.dumps(processed_payload)\n    except TypeError as e:\n        print(f\"[Telemetry ERROR]: Payload for task_id {task_id}, event_type {event_type} is not JSON serializable: {e}\")\n        processed_payload = {\"error\": f\"Non-serializable payload: {str(e)}\", \"original_payload_summary\": str(payload)[:200]} # Truncate for safety\n\n    event = {\n        \"event_id\": _generate_event_id(),\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n        \"schema_version\": \"1.0.0\", # This should match your current schema version\n        \"musician_name\": musician_name,\n        \"task_id\": task_id,\n        \"event_type\": event_type,\n        \"payload\": processed_payload,\n        \"data_hash\": _calculate_payload_hash(processed_payload),\n        \"parent_event_id\": parent_event_id,\n        \"sensitive_data_masked\": was_masked\n    }\n\n    # Dynamic schema selection for validation\n    schema_to_validate = None\n    if event_type in [\"task_start\", \"task_progress\", \"task_complete\", \"task_error\"]:\n        schema_to_validate = TASK_LIFECYCLE_EVENT_SCHEMA\n    elif event_type == \"gdc_snapshot\":\n        schema_to_validate = GDC_SNAPSHOT_EVENT_SCHEMA\n    elif event_type == \"sub_log_entry\":\n        schema_to_validate = SUB_LOG_ENTRY_SCHEMA\n    else:\n        print(f\"[Telemetry WARNING]: No specific schema found for event_type '{event_type}'. Proceeding without validation.\")\n        # For unknown event types, we might still want to emit but flag it.\n        schema_to_validate = {} # Empty schema means no validation, but still allows processing\n\n    if schema_to_validate: # Only attempt validation if a schema was found/loaded\n        try:\n            validate(instance=event, schema=schema_to_validate, resolver=_get_schema_resolver())\n        except ValidationError as e:\n            print(f\"[Telemetry SCHEMA ERROR]: Event validation failed for task {task_id}, type {event_type}: {e.message}\")\n            # print(f\"Invalid event data: {json.dumps(event, indent=2)}\") # Uncomment for debugging invalid data\n            # Do NOT add invalid event to buffer if strict validation is desired\n            return\n        except Exception as e:\n            print(f\"[Telemetry SCHEMA ERROR]: Unexpected error during validation for {task_id}, type {event_type}: {e}\")\n            return\n\n    async with _buffer_lock:\n        _telemetry_buffer.append(event)\n        if len(_telemetry_buffer) >= TELEMETRY_BUFFER_SIZE:\n            # Trigger immediate flush if buffer is full\n            asyncio.create_task(_flush_telemetry_buffer())\n\n# --- Decorator Implementation ---\ndef log_task_lifecycle(event_type_map=None, mask_sensitive_params=True):\n    \"\"\"\n    Decorator to log the lifecycle events of a task.\n    Supports 'task_start', 'task_progress', 'task_complete', 'task_error'.\n    Can also be used for 'task_progress' or 'sub_log_entry' within the decorated function.\n\n    Args:\n        event_type_map (dict, optional): A dictionary mapping a method's outcome\n                                         (e.g., 'start', 'success', 'failure')\n                                         to specific event types.\n                                         Defaults to {'start': 'task_start', 'success': 'task_complete', 'failure': 'task_error'}.\n        mask_sensitive_params (bool): If True, attempts to mask sensitive data in parameters.\n    \"\"\"\n    if event_type_map is None:\n        event_type_map = {\n            'start': 'task_start',\n            'success': 'task_complete',\n            'failure': 'task_error'\n        }\n\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(instance, *args, **kwargs): # 'instance' is the MusicianProcess object\n                                                     # *args and **kwargs are the parameters for func (e.g., action_name, parameters, task_id_for_decorator)\n            musician_name = instance.name\n            task_id = kwargs.get('task_id') or getattr(instance, '_current_task_id', 'unknown_task')\n\n            # Extract relevant parameters for logging from *args and **kwargs passed to the wrapper\n            # These are the arguments that the decorated function (e.g., _execute_decorated_action) expects\n            log_parameters = {}\n            # Assuming the decorated function's signature is _execute_decorated_action(self, action_name, parameters_dict, task_id_for_decorator)\n            # So, action_name is args[0], parameters_dict is args[1], task_id_for_decorator is args[2]\n            if len(args) > 0:\n                log_parameters['action_name'] = args[0]\n            if len(args) > 1 and isinstance(args[1], dict):\n                log_parameters.update(args[1]) # Merge the parameters dictionary directly\n            log_parameters.update(kwargs) # Include any kwargs from the call\n\n            start_time = time.time()\n\n            # START EVENT\n            start_event_payload = {\n                \"method\": func.__name__,\n                \"args\": log_parameters, # Will be masked by emit_telemetry_event\n                \"description\": f\"Starting execution of {func.__name__} for task {task_id}.\",\n                \"status\": \"in_progress\"\n            }\n            await emit_telemetry_event(\n                musician_name,\n                task_id,\n                event_type_map['start'],\n                start_event_payload,\n                mask_sensitive=mask_sensitive_params\n            )\n\n            try:\n                # *** CORRECTED LINE ***\n                # The original function (func) is called with the instance ('self') passed explicitly,\n                # followed by the rest of its arguments.\n                result = await func(instance, *args, **kwargs)\n\n                # COMPLETE EVENT\n                complete_event_payload = {\n                    \"method\": func.__name__,\n                    \"status\": \"success\",\n                    \"result_summary\": str(result)[:500] if result is not None else \"No explicit result.\", # Summarize result\n                    \"duration_ms\": (time.time() - start_time) * 1000 # Add actual duration\n                }\n                await emit_telemetry_event(\n                    musician_name,\n                    task_id,\n                    event_type_map['success'],\n                    complete_event_payload,\n                    mask_sensitive=mask_sensitive_params\n                )\n                return result\n\n            except Exception as e:\n                # ERROR EVENT\n                error_event_payload = {\n                    \"method\": func.__name__,\n                    \"status\": \"failure\",\n                    \"error_type\": type(e).__name__,\n                    \"error_message\": str(e),\n                    \"duration_ms\": (time.time() - start_time) * 1000, # Add actual duration\n                    \"stack_trace\": traceback.format_exc() # Capture full stack trace\n                }\n                await emit_telemetry_event(\n                    musician_name,\n                    task_id,\n                    event_type_map['failure'],\n                    error_event_payload,\n                    mask_sensitive=mask_sensitive_params\n                )\n                raise # Re-raise the exception to not alter original function behavior\n\n        return wrapper\n    return decorator\n"
    },
    "dependencies": ["task_read_telemetry"]
  },
  "task_read_conductor": {
    "musician": "FileSystemMusician",
    "action": "read_file",
    "parameters": {
      "file_path": "C:\\syncphony\\conductor.py"
    }
  },
  "task_fix_and_write_conductor": {
    "musician": "FileSystemMusician",
    "action": "write_file",
    "parameters": {
      "file_path": "C:\\syncphony\\conductor.py",
      "content": "# C:\\syncphony\\conductor.py\n\nimport json\nimport sys\nimport time\nimport multiprocessing\nimport asyncio\nimport os\nfrom datetime import datetime\nimport queue\n\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom telemetry import emit_telemetry_event, _telemetry_flusher_task\nfrom genome_data_cache import GenomeDataCache\n\n\ndef main(symphony_path, task_queues, reporting_queue, input_queue, log_queue):\n    \"\"\"\n    Main entry point for the Conductor Process.\n    Initializes its own asyncio event loop for GDC heartbeat and async operations.\n    \"\"\"\n    log_queue.put(\"[Conductor]: Process started, initializing asyncio loop.\")\n\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n\n    conductor = Conductor(symphony_path, task_queues, reporting_queue, input_queue, log_queue)\n\n    try:\n        loop.run_until_complete(conductor.run_conductor_async_loop())\n    finally:\n        loop.close()\n        log_queue.put(\"[Conductor]: Asyncio loop closed. Process shutting down.\")\n\n\nclass Conductor:\n    def __init__(self, symphony_path, task_queues, reporting_queue, input_queue, log_queue):\n        self.symphony_path = symphony_path\n        self.task_queues = task_queues\n        self.reporting_queue = reporting_queue\n        self.input_queue = input_queue\n        self.log_queue = log_queue\n        self.symphony_data = None\n        self.gdc = GenomeDataCache()\n        self.conductor_name = \"Conductor\"\n        self.current_symphony_id = None\n\n    async def run_conductor_async_loop(self):\n        \"\"\"The main asynchronous loop for the Conductor.\"\"\"\n        self.log_queue.put(\"[Conductor DEBUG]: Entering run_conductor_async_loop.\")\n\n        asyncio.create_task(_telemetry_flusher_task())\n        self.log_queue.put(\"[Conductor DEBUG]: Conductor's telemetry flusher task started (from run_conductor_async_loop).\")\n\n        asyncio.create_task(self._gdc_heartbeat_task())\n        self.log_queue.put(\"[Conductor DEBUG]: GDC heartbeat task started (from run_conductor_async_loop).\")\n\n        self.log_queue.put(\"[Conductor DEBUG]: Before symphony load call.\")\n        self.symphony_data = await asyncio.to_thread(self.load_symphony)\n        self.log_queue.put(\"[Conductor DEBUG]: After symphony load call. Symphony data is: \" + (\"None\" if self.symphony_data is None else \"Loaded\"))\n\n        if not self.symphony_data:\n            self.log_queue.put(\"[Conductor]: Halting due to error loading symphony.\")\n            # FIX: Added 'description' to payload as required by schema\n            await emit_telemetry_event(self.conductor_name, \"symphony_load_error\", \"task_error\",\n                                       {\"method\": \"load_symphony\", \"status\": \"failure\", \"path\": self.symphony_path,\n                                        \"error_message\": \"Symphony load failed.\", \"description\": \"Failed to load symphony file from disk.\"})\n            return\n\n        self.log_queue.put(\"[Conductor DEBUG]: Before first emit_telemetry_event for symphony_start.\")\n        # FIX: Added 'description' to payload as required by schema (even if part of string)\n        await emit_telemetry_event(self.conductor_name, self.current_symphony_id, \"task_start\",\n                                   {\"method\": \"symphony_execution\", \"status\": \"in_progress\",\n                                    \"description\": f\"Starting Symphony: {self.symphony_path}\"})\n        self.log_queue.put(\"[Conductor DEBUG]: After first emit_telemetry_event for symphony_start.\")\n\n        self.log_queue.put(\"[Conductor DEBUG]: Before dispatch_and_wait call.\")\n        await self.dispatch_and_wait(self.symphony_data)\n        self.log_queue.put(\"[Conductor DEBUG]: After dispatch_and_wait call.\")\n\n\n        # Emit a \"symphony_complete\" event\n        # FIX: Added 'description' and 'result_summary' to payload as required by schema\n        await emit_telemetry_event(self.conductor_name, self.current_symphony_id, \"task_complete\",\n                                   {\"method\": \"symphony_execution\", \"status\": \"success\",\n                                    \"description\": \"Symphony performance completed.\", \"result_summary\": \"All tasks processed according to plan.\"})\n        self.log_queue.put(\"[Conductor DEBUG]: After final emit_telemetry_event for symphony_complete.\")\n\n\n        # Keep the Conductor process alive after Symphony completion to handle reports or further commands\n        self.log_queue.put(\"[Conductor]: Symphony performance finished. Conductor awaiting further commands or stop signal.\")\n        while True:\n            try:\n                command = await asyncio.to_thread(self.input_queue.get, timeout=1)\n                if command == 'STOP':\n                    self.log_queue.put(\"[Conductor]: Received STOP signal. Shutting down.\")\n                    break\n            except queue.Empty:\n                await asyncio.sleep(0.1)\n            except Exception as e:\n                self.log_queue.put(f\"[{self.conductor_name}]: Error reading input queue: {e}\")\n                await asyncio.sleep(1)\n\n    def load_symphony(self):\n        self.log_queue.put(f\"[{self.conductor_name}]: Loading Symphony from '{self.symphony_path}'...\")\n        try:\n            with open(self.symphony_path, 'r') as f:\n                symphony = json.load(f)\n            self.log_queue.put(f\"[{self.conductor_name}]: Symphony loaded successfully.\")\n\n            symphony_id = f\"symphony_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}_{os.urandom(4).hex()}\"\n            self.gdc.update_data(\"symphony_meta\", symphony_id, {\n                \"path\": self.symphony_path,\n                \"loaded_at\": datetime.utcnow().isoformat() + \"Z\",\n                \"total_tasks\": len(symphony),\n                \"status\": \"loaded\"\n            })\n            self.current_symphony_id = symphony_id\n\n            initial_task_states = {\n                task_id: {\"status\": \"pending\", \"dependencies\": task.get(\"dependencies\", []), \"musician\": task.get(\"musician\")}\n                for task_id, task in symphony.items()\n            }\n            for task_id, state in initial_task_states.items():\n                self.gdc.update_data(\"tasks\", task_id, state)\n\n            self.gdc.get_merkle_root()\n\n            return symphony\n        except FileNotFoundError:\n            self.log_queue.put(f\"[{self.conductor_name} ERROR]: Symphony file not found: '{self.symphony_path}'\")\n            return None\n        except json.JSONDecodeError as e:\n            self.log_queue.put(f\"[{self.conductor_name} ERROR]: Invalid JSON in Symphony file: {e}\")\n            return None\n        except Exception as e:\n            self.log_queue.put(f\"[{self.conductor_name} ERROR]: An unexpected error occurred loading symphony: {e}\")\n            return None\n\n    async def dispatch_and_wait(self, symphony):\n        self.log_queue.put(f\"[{self.conductor_name} DEBUG]: Entering dispatch_and_wait.\")\n\n        all_task_ids = set(symphony.keys())\n        completed_tasks = set()\n        dispatched_tasks = set()\n\n        def is_eligible(task_id_check):\n            deps = symphony[task_id_check].get('dependencies', [])\n            if not deps:\n                return task_id_check not in dispatched_tasks\n\n            for dep_id in deps:\n                dep_data = self.gdc.get_data(\"tasks\", dep_id)\n                if not dep_data or dep_data.get(\"status\") != \"completed\":\n                    return False\n            \n            return task_id_check not in dispatched_tasks\n\n\n        for task_id in all_task_ids:\n            if is_eligible(task_id):\n                musician_name = symphony[task_id].get('musician')\n                if musician_name and musician_name in self.task_queues:\n                    task_package = {'task_id': task_id, 'details': symphony[task_id]}\n                    await asyncio.to_thread(self.task_queues[musician_name].put, task_package)\n                    dispatched_tasks.add(task_id)\n\n                    self.gdc.update_data(\"tasks\", task_id, {\"status\": \"dispatched\", \"musician\": musician_name})\n\n                    self.log_queue.put(f\"[{self.conductor_name}]: Dispatched task '{task_id}' to '{musician_name}'.\")\n                else:\n                    self.log_queue.put(f\"[{self.conductor_name}]: WARNING! Skipping task '{task_id}' due to invalid or missing musician.\")\n                    self.gdc.update_data(\"tasks\", task_id, {\"status\": \"skipped\", \"reason\": \"invalid_musician\"})\n                    completed_tasks.add(task_id)\n\n        expected_total_tasks_to_resolve = len(all_task_ids)\n        \n        self.log_queue.put(f\"[{self.conductor_name}]: Monitoring {expected_total_tasks_to_resolve} tasks in total...\")\n\n        processed_tasks_for_loop_exit = completed_tasks.copy()\n        processed_tasks_for_loop_exit.update(dispatched_tasks)\n\n        while len(processed_tasks_for_loop_exit) < expected_total_tasks_to_resolve:\n            try:\n                report = await asyncio.to_thread(self.reporting_queue.get, timeout=0.1)\n                task_id = report.get('task_id')\n                status = report.get('status')\n                error = report.get('error')\n\n                task_data = self.gdc.get_data(\"tasks\", task_id)\n                if task_data:\n                    task_data[\"status\"] = status\n                    if error:\n                        task_data[\"error\"] = error\n                    self.gdc.update_data(\"tasks\", task_id, task_data)\n\n                if task_id not in processed_tasks_for_loop_exit:\n                    processed_tasks_for_loop_exit.add(task_id)\n                    completed_tasks.add(task_id)\n                    self.log_queue.put(f\"[{self.conductor_name}]: Received completion report for '{task_id}' (Status: {status}). ({len(processed_tasks_for_loop_exit)}/{expected_total_tasks_to_resolve} resolved)\")\n\n                    for tid in list(all_task_ids):\n                        if tid in processed_tasks_for_loop_exit or tid in dispatched_tasks:\n                            continue\n\n                        if is_eligible(tid):\n                            musician_name = symphony[tid].get('musician')\n                            if musician_name and musician_name in self.task_queues:\n                                task_package = {'task_id': tid, 'details': symphony[tid]}\n                                await asyncio.to_thread(self.task_queues[musician_name].put, task_package)\n                                dispatched_tasks.add(tid)\n                                processed_tasks_for_loop_exit.add(tid)\n                                \n                                self.gdc.update_data(\"tasks\", tid, {\"status\": \"dispatched\", \"musician\": musician_name})\n\n                                self.log_queue.put(f\"[{self.conductor_name}]: Task '{tid}' now eligible and dispatched to '{musician_name}' after dependencies met.\")\n                            else:\n                                self.log_queue.put(f\"[{self.conductor_name}]: WARNING! Skipping newly eligible task '{tid}' due to invalid or missing musician.\")\n                                self.gdc.update_data(\"tasks\", tid, {\"status\": \"skipped\", \"reason\": \"invalid_musician_dependency\"})\n                                completed_tasks.add(tid)\n                                processed_tasks_for_loop_exit.add(tid)\n\n\n                        else:\n                            deps = symphony[tid].get('dependencies', [])\n                            if deps:\n                                all_deps_resolved = True\n                                any_dep_failed = False\n                                for dep_id in deps:\n                                    dep_data = self.gdc.get_data(\"tasks\", dep_id)\n                                    if not dep_data or dep_data.get(\"status\") == \"pending\" or dep_data.get(\"status\") == \"dispatched\":\n                                        all_deps_resolved = False\n                                    elif dep_data.get(\"status\") == \"failed\":\n                                        any_dep_failed = True\n\n                                if all_deps_resolved and any_dep_failed:\n                                    self.log_queue.put(f\"[{self.conductor_name}]: Skipping task '{tid}' due to failed dependency/dependencies.\")\n                                    self.gdc.update_data(\"tasks\", tid, {\"status\": \"skipped\", \"reason\": \"failed_dependency\"})\n                                    completed_tasks.add(tid)\n                                    processed_tasks_for_loop_exit.add(tid)\n\n\n            except queue.Empty:\n                await asyncio.sleep(0.05)\n            except Exception as e:\n                self.log_queue.put(f\"[{self.conductor_name} ERROR]: Error processing report queue or dispatching: {e}\")\n                await asyncio.sleep(1)\n\n        pending_tasks_final = [tid for tid in all_task_ids if tid not in processed_tasks_for_loop_exit]\n        if pending_tasks_final:\n            self.log_queue.put(f\"[{self.conductor_name}]: Warning: {len(pending_tasks_final)} tasks remain pending (possible logic error, self-looping dependency, or unhandled scenario).\")\n            for tid in pending_tasks_final:\n                if self.gdc.get_data(\"tasks\", tid).get(\"status\") == \"pending\":\n                    self.gdc.update_data(\"tasks\", tid, {\"status\": \"stuck\", \"reason\": \"unresolved_dependencies_or_error_loop\"})\n        \n        self.log_queue.put(f\"[{self.conductor_name}]: All expected tasks processed. Performance finished.\")\n        self.gdc.get_merkle_root()\n\n    # *** CORRECTED METHOD PLACEMENT ***\n    # This method is now correctly indented to be part of the Conductor class.\n    async def _gdc_heartbeat_task(self):\n        \"\"\"Periodically captures and logs GDC snapshots.\"\"\"\n        while True:\n            # self.gdc.SNAPSHOT_INTERVAL_SECONDS is now an instance attribute\n            await asyncio.sleep(self.gdc.SNAPSHOT_INTERVAL_SECONDS)\n            current_root = self.gdc.get_merkle_root()\n\n            previous_root = None\n            if len(self.gdc.get_root_history()) >= 2:\n                previous_root = self.gdc.get_root_history()[-2][0]\n\n            self.log_queue.put(f\"[{self.conductor_name} GDC]: Snapshot taken. Merkle Root: {current_root}\")\n\n            await emit_telemetry_event(\n                self.conductor_name,\n                \"gdc_snapshot_system\",\n                \"gdc_snapshot\",\n                {\"merkle_root\": current_root, \"previous_root\": previous_root,\n                 \"description\": \"GDC snapshot taken by heartbeat task.\"}, # Added description\n                mask_sensitive=False\n            )\n\n            if previous_root and self.gdc.detect_gdc_changes(previous_root, current_root):\n                self.log_queue.put(f\"[{self.conductor_name} GDC]: Detected change in GDC state!\")\n                await emit_tele